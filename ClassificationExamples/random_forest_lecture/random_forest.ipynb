{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will use the Iris flower dataset from sklearn to introduce classification with Random Forest.\n",
    "\n",
    "Key distinctions between decision tree and random forest are explored. \n",
    "\n",
    "Feature importance and partial dependency plots for random forest are generated post-training to facilitate model visibility. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "- Set up train-test data\n",
    "- Review decision trees\n",
    "- Visualize a decision tree\n",
    "- Introduction to random forest\n",
    "- Train and predict with a random forest\n",
    "- Visualize feature importances\n",
    "- Create partial dependency plots for random forest\n",
    "- Knowledge check and questions\n",
    "- Additional EDA & Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite setup and knowledge\n",
    "- Correct Python environment\n",
    "- Train-test split\n",
    "- Classification metrics\n",
    "- Decision Trees\n",
    "- Measures of node impurity (Shannon Entropy and Gini Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "1. Apply a random forest classifier to a dataset\n",
    "1. Visualize feature importances from a trained random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "import skimpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "df['label'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[data.feature_names], \n",
    "    df['label'], \n",
    "    random_state=42,\n",
    "    stratify=df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first build a decision tree model to review their structure before moving on to random forest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(dt, \n",
    "                   feature_names=data.feature_names,  \n",
    "                   class_names=data.target_names,\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "def print_metrics(y_test, y_pred):\n",
    "    \"\"\"function to display model metrics in a human readable fashion\n",
    "    y_test: Array of true y values\n",
    "    y_pred: Arrah of predicted y values\n",
    "    \"\"\"\n",
    "    scores = [accuracy_score, recall_score, precision_score, f1_score]\n",
    "    s_labels = ['Accuracy', 'Recall', 'Precision', 'F1']\n",
    "    for score, s_label in zip(scores, s_labels):\n",
    "        if s_label == 'Accuracy':\n",
    "            print(f\"{s_label}: {score(y_test, y_pred): .2f}\")\n",
    "        else:\n",
    "            print(f\"{s_label}: {score(y_test, y_pred, average='weighted'):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm_disp = ConfusionMatrixDisplay.from_predictions(y_true=y_test,y_pred=predicted_labels, cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Knowledge\n",
    "1. Are decision trees deterministic?\n",
    "    - Yes they are deterministic. The best split will be found at each iterative step and will be used. \n",
    "1. How are decision trees split determined?\n",
    "    - Information gain or entropy reduction\n",
    "1. Are decision trees parametric? \n",
    "    - No, they are not parametric. Splits may differ in direction based on values.\n",
    "1. Decision trees often have high variance, why might that be?\n",
    "    - May split on wrong features or overfit to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From tree to forest\n",
    "\n",
    "1. How might multiple decision trees be leveraged to reduce variance?\n",
    "    1. Create multiple classifiers and average the results. Multiple weak learners can ofter produce a strong learner.\n",
    "1. Would multiple deterministic decision trees be useful?\n",
    "    1. Only if they were NOT deterministic.\n",
    "1. How could they not be deteministic?\n",
    "    1. Bootstrapping data and limiting features at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "- An ensemble method that combines many decision trees which have been given different subsets of the data and features to create a strong learner \n",
    "- Decision made on majority vote\n",
    "- Reduces variance and creates a non-deterministic model\n",
    "- Generally use a large number of bushy trees\n",
    "- Can get excellent performance with minimum tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=1)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_disp = ConfusionMatrixDisplay.from_predictions(y_true=y_test, y_pred=rf_preds, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing trees in the forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(est_num=0):\n",
    "    fn=data.feature_names\n",
    "    cn=data.target_names\n",
    "    tree.plot_tree(rf.estimators_[est_num],\n",
    "                   feature_names = fn, \n",
    "                   class_names = cn,\n",
    "                   filled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(est_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(est_num=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing only on the first split in the two trees above, we can see differences in the splits used to build the forest.\n",
    "\n",
    "The first one split on sepal lenth <= 5.35 and the second on petal length <=2.45. The depth of the trees also varies. This is due to the randomness induced in the trees with bootstrapping and feature selection. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_import = pd.DataFrame()\n",
    "ft_import['Features'] = data.feature_names\n",
    "ft_import['Importance'] = rf.feature_importances_\n",
    "ft_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All importances sum to 1\n",
    "ft_import.Importance.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_import.plot.bar(x='Features', y='Importance', rot=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Dependency Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows the marginal effect of a feature on predictions.\n",
    "\n",
    "Shows effect of predictions when all observations have a feature set to a particular value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "pd_plot = PartialDependenceDisplay.from_estimator(rf, X_train, ['petal length (cm)'], target='setosa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in data.target_names:\n",
    "    print('Target is: ', target)\n",
    "    PartialDependenceDisplay.from_estimator(rf, X_train, data.feature_names, target=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantages of Random Forests\n",
    "\n",
    "1. Ensemble model (Wisdom of the Crowd)\n",
    "1. Good out-of-box performance\n",
    "1. Multiple trees can be trained at once "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cons of Random Forests\n",
    "1. Expensive to train\n",
    "2. Can produce very large model files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison and evaluation\n",
    "\n",
    "1. Which model performed better?\n",
    "2. Which feature had the most influence on the random forest model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Check\n",
    "1. A random forest is the same as combining many decision trees?\n",
    "1. Name two ways in which random forests are made non-deterministic. \n",
    "1. Random forest classifiers are parametric?\n",
    "1. Name two visuals to help intrepret random forest models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Objectives \n",
    "1. Apply a random forest classifier to a dataset.\n",
    "1. Visualize feature importances from a trained random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "1. Tune the random forest classifier\n",
    "    1. Number of estimators\n",
    "    1. Criterion: default is gini, can also try entropy\n",
    "    1. Max depth, min samples\n",
    "    1. Number of features\n",
    "1. Improve visuals\n",
    "   1. Add title to partial dependency plots\n",
    "   1. Create interactive visuals with Plotly for scatter and bar plots below\n",
    "   1. Edit layout so all axis labels are fully visible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Day Activities\n",
    "1. Code random forest from scratch\n",
    "1. Code partial dependecy plot function from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional EDA & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimpy.skim(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns[:4]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='sepal length (cm)', y='sepal width (cm)', hue='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='petal length (cm)', y='petal width (cm)', hue='label')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
