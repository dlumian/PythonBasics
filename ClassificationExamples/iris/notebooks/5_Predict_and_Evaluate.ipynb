{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "Many metrics can be used to determine the quality of a classification model.\n",
    "\n",
    "Evaluation should be decided based on test data performance. However, stats on the training data may also be helpful for determining under or over fitting.\n",
    "\n",
    "This notebook will run metrics on both datasets, but **test metrics** are the primary focus to evaluate model generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('..', 'data')\n",
    "# training data\n",
    "X_train_path = os.path.join(data_path, 'X_train.csv')\n",
    "y_train_path = os.path.join(data_path, 'y_train.csv')\n",
    "# test data\n",
    "X_test_path = os.path.join(data_path, 'X_test.csv')\n",
    "y_test_path = os.path.join(data_path, 'y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(X_train_path)\n",
    "y_train = pd.read_csv(y_train_path)\n",
    "\n",
    "X_test = pd.read_csv(X_test_path)\n",
    "y_test = pd.read_csv(y_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = os.path.join('..', 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_path = os.path.join(models_path, 'lr.pkl')\n",
    "lr = load(lr_model_path)\n",
    "lr_train_preds = lr.predict(X_train)\n",
    "lr_test_preds = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_path = os.path.join(models_path, 'rf.pkl')\n",
    "rf = load(rf_model_path)\n",
    "rf_train_preds = rf.predict(X_train)\n",
    "rf_test_preds = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = os.path.join('..', 'imgs')\n",
    "os.makedirs(images_path, exist_ok=True)\n",
    "\n",
    "lr_train_cm_path = os.path.join(images_path, 'lr_train_cm.png')\n",
    "rf_train_cm_path = os.path.join(images_path, 'rf_train_cm.png')\n",
    "\n",
    "lr_test_cm_path = os.path.join(images_path, 'lr_test_cm.png')\n",
    "rf_test_cm_path = os.path.join(images_path, 'rf_test_cm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of preds and paths\n",
    "true_list = [y_train, y_test, y_train, y_test]\n",
    "pred_list = [lr_train_preds, lr_test_preds, rf_train_preds, rf_test_preds]\n",
    "path_list = [lr_train_cm_path, lr_test_cm_path, rf_train_cm_path, rf_test_cm_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trues, preds, path in zip(true_list, pred_list, path_list):\n",
    "    cm_disp = ConfusionMatrixDisplay.from_predictions(y_true=trues, y_pred=preds, cmap='Blues')\n",
    "    print(f'Saving image to path: {path}.')\n",
    "    cm_disp.figure_.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Logistic Regression Accuracy Score: {accuracy_score(y_test.target, lr_test_preds): .2f} ')\n",
    "print(f'Random Forest Accuracy Score: {accuracy_score(y_test.target, rf_test_preds): .2f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test.target, lr_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test.target, rf_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
